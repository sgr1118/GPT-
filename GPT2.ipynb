{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbrde0niNvJNV7M7zX8W3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/GPT-/blob/main/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT(Generative Pre-Training Transformer)\n",
        "\n",
        "GPT는 트랜스포머의 decoder 구조만을 이용하여 만든 네트워크입니다. 트랜스포머의 decoder를 아주 깊고 깊게 쌓아 많은 데이터를 학습 시켜 성능을 높힌 네크워크다.\n",
        "\n",
        "1. GPT의 구조\n",
        "\n",
        "Decoder만을 이용했다는 것을 이해하기위해 트랜스포머의 Decoder를 먼저 봐야할 필요가 있다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/10_egROfLK.png)\n",
        "<center>트랜스포머 Deocder</center>\n",
        "\n",
        "- Decoder는 masked Multi-Head Attention, Multi-Head Attention, Feed Forward Neural Network로 이루어져 있다. 바로 이 구조를 차용하여 Decoder block를 많이 쌓아 올리면 GPT가 된다."
      ],
      "metadata": {
        "id": "eIKKOUfrU4QE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT를 두 부분으로 나누어서 좀 더 자세히 보면 아래와 같습니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/11_HNGuRhf.png)\n",
        "<center>GPT의 모델 구조</center>\n",
        "\n",
        "Transfomer Decoder Block : Pretraining LM (Unspervised Learning)\n",
        "\n",
        "- 첫 번째로 볼 부분은 빨간색 박스 안이다. pretrain 하는 부분으로 unsupervised learning(비지도 학습)을 하게 됩니다. 위에 트랜스포머의 구조 그대로다\n",
        "\n",
        "1. Embedding\n",
        "- GPT는 텍스트의 Embedding으로 BPE(Byte-pair Encoding)를 사용하고 있다.\n",
        "- BPE는 모든 단어를 문자(바이트)들의 집합으로 취급하여 자주 등장하는 문자 쌍을 합치는 subword tokeniztion이었다. 처음보는 단어일지라도 문자(알파벳)들의 조합으로 나타내어 OOV 문제를 해결할 수 있다는 장점이 있다. 기존 트랜스포머와 마찬가지로 position encoding(포지션 인코딩)도 함께 사용한다.\n",
        "\n",
        "2. Masked Multi-Head Attention\n",
        "- Masked Multi-Head Attention은 모든 것을 병렬적으로 처리하는 트랜스포머에 자기회귀적인(Autoregressive)인 특성을 부여하기 위해 만든 장치이다. 여기서 자기회귀적이라 함은 훈련 단계에서 디코더에게 정답 문장을 매 스텝 단위로 단어 하나씩 알려주고 그 다음 단어를 예측(Next Token Prediction)하게 하는 형태로 학습되는 형태하는 뜻이다. 이는 마치 sequence-to-sequence 모델에서 디코더가 번역 문장을 생성할 때 time-step을 하나하나 거치듯이 만들어주는 것입니다. 순차처리 방식의 RNN과 달리 정답 문장의 모든 단어를 한꺼번에 입력받는 트랜스포머의 deocder는 학습할 때 현재 자기보다 미래에 생성될 토큰을 보지 못하도록 masking이 필요하게 됩니다.\n",
        "\n",
        "- 이 구조는 우리가 학습했던 언어 모델과 같은 구조입니다. 다시 한번 되짚어 보자면, 언어 모델은 비지도 학습을 통해 문장의 자연스러운 순서를 학습하게 됩니다. 그래서 GPT는 문자 생성에 매우 특화되어 있다. GPT를 연구한 Open Ai도 너무나 자연스러운 문장을 만들어내서 악용이 우려하고 있다.\n",
        "\n",
        "- Text Prediction & Text classification: finetuning downstream task (Supervised Learning)\n",
        "\n",
        "- pretrain이 끝나게 되면 GPT는 downstream task에 맞게 finetuning을 하게 됩니다. 바로 파란색 박스 부분에 해당되는 부분이다. 여기서 우리는 기존에 봤던 모델들과 조금 다른 점을 발견할 수 있다. 바로 두개의 Objective가 존재한다는 것이다.\n",
        "\n",
        "- 말 그대로 모델이 두 가지의 문제를 동시에 푸는 겁니다. text prediction과 text classification이 각각 다른 모델들을 이용하여 output을 만들어내는 것이 아니라 한 모델에서 동시에 output을 내는 겁니다.\n",
        "\n",
        "- 논문의 저자들은 이렇게 실제로 하는 문제인 주요 task와 동시에 보조적으로 또 다른 문제를 풀 때 (Auxiliary objective)주요 task에 대한 정확도가 더 올라갔음을 확인했다.\n",
        "\n",
        "- 생각해보면 LM또한, auxiliary로 얻어진 결과라고 생각할 수 있습니다. 시퀀스의 다음 나올 단어들을 학습하다 보니 전체적인 언어의 구조를 알게 된 것이다.\n",
        "\n",
        "- GPT의 전체 코드는 약간 복잡해 보일 수 있지만, TFGPT2MainLayer라는 전체 모델 클래스 안에서 TFBlock 레이어 클래스를 반복해서 사용하고 있는 부분을 잘 봐야한다. TFBlock 클래스 안에서 TFAtttion, TFMLP 레이어가 사용되는 구조가 위에서 소개한 GPT의 모델 구조 그림에 표현되어 있다."
      ],
      "metadata": {
        "id": "KyDycDULWXVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Transformation\n",
        "\n",
        "- 모델이 한 개인데 어떻게 classification, entailment 등등 다양한 문제를 풀 수 있을까요?\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/12_m2lKyFg.max-800x600.png)\n",
        "<center>GPT task</center>\n",
        "\n",
        "- 바로 input을 변형시켜서 입니다.\n",
        "\n",
        "- GPT는 언어 모델이기 때문에 pretrain시에는 문장(단어의 시퀀스들)을 그대로 input을 줍니다. finetuning시에도 똑같이 단어의 시퀀스들을 주면 된다. 이때, input을 아주 조금만 변형시켜 주면 됩니다.\n",
        "\n",
        "- 예를 들어 classification task를 풀기 위해 finetuning을 하게 된다면 start, input text, extract, class 이렇게 구성된 데이터셋을 학습시키면 된다. GPT는 이 데이터셋에 맞추어 weight들을 조정하게 될 것이다.\n",
        "\n",
        "- finetuning이 끝나고 테스트 시에는  start, input text, extract을 input으로 주면 해당 시퀀스에 뒤이어 나올 토큰 즉 class를 생성하게 되는 것이다."
      ],
      "metadata": {
        "id": "pKB9mZ33yc2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch를 기반으로한 GPT Model 구현"
      ],
      "metadata": {
        "id": "gBhOSUubH_Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "-MpxBU5pJG0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Config\n",
        "- Transformer와 파라미터를 동일하게 설정\n",
        "- GPT는 Decoder만 사용하므로 Encoder 부분은 제거한다.\n",
        "- [config.json](https://github.com/paul-hyun/transformer-evolution/blob/master/gpt/config.json)"
      ],
      "metadata": {
        "id": "BRe8oz5aIbai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config({\n",
        "    \"n_dec_vocab\": len(vocab),\n",
        "    \"n_dec_seq\": 256,\n",
        "    \"n_layer\": 6,\n",
        "    \"d_hidn\": 256,\n",
        "    \"i_pad\": 0,\n",
        "    \"d_ff\": 1024,\n",
        "    \"n_head\": 4,\n",
        "    \"d_head\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"layer_norm_epsilon\": 1e-12\n",
        "})\n",
        "print(config)"
      ],
      "metadata": {
        "id": "eUjsM8y9IEwB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "bca72f4b-05de-49fc-c03e-db68726efa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-dc6a2eac5a69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m config = Config({\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"n_dec_vocab\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"n_dec_seq\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"n_layer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"d_hidn\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Decoder\n",
        "\n",
        "![](https://paul-hyun.github.io/assets/2019-12-30/decoder.png)\n",
        "<center>Decoder</center>\n",
        "\n",
        "- GPT는 표준 Transformer의 Encoder는 사용하지 않고 Decoder만 사용하므로 Decoder에서 Encoder의 출력과 Attention을 하는 부분인 Encoder-Decoder Multi-Head Attention 부분을 제거해야 한다. 나머지 부분은 Transformer와 동일하다.\n"
      ],
      "metadata": {
        "id": "LDRfc_Q_JXee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Layer\n",
        "- 표준 Transformer DecoderLayer에서 Encoder-Decoder Multi-Head Attention을 제거한 코드 입니다."
      ],
      "metadata": {
        "id": "lqLXikqOKCt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" decoder layer \"\"\"\n",
        "class Decoderlayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.self_attn = MultiheadAttention(self.config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
        "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, dec_inputs, self_attn_mask):\n",
        "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq)\n",
        "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
        "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\n",
        "        # (bs, n_dec_seq, d_hidn)\n",
        "        ffn_outputs = self.pos_ffn(self_att_outputs)\n",
        "        ffn_outputs = self.layer_norm3(self-att_outputs + ffn_outputs)\n",
        "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\n",
        "        return ffn_outputs, self_attn_prob    "
      ],
      "metadata": {
        "id": "R38nF7fgJWxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "- 표준 Transformer Decoder에서 Encoder출력을 DecoderLayer에 입력하는 부분을 제거한 코드 입니다."
      ],
      "metadata": {
        "id": "XKoS9FRsNco6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" decoder \"\"\"\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init()\n",
        "        self.config = config\n",
        "\n",
        "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
        "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config,n_layer)])\n",
        "    \n",
        "    def forward(self, dec_inputs):\n",
        "        positions = torch.arange(dec_inputs.size(1), device = dec_inputs.device, dtype = dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
        "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
        "        positions.masked_fill_(pos_mask, 0)\n",
        "\n",
        "        # (bs, n_dec_seq, d_hidn)\n",
        "        dec_outputs = self.dec_emb(dec_imputs) + self.pos_emb(positions)\n",
        "\n",
        "        # (bs, n_dec_seq, n_dec_seq)\n",
        "        dec_attn_pad_mask = get_attn_apd_mask(dec_inputs, dec_inputs, self.config.i_pad)\n",
        "        # (bs, n_dec_seq, n_dec_seq)\n",
        "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\n",
        "        # (bs, n_dec_seq, n_dec_seq)\n",
        "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
        "\n",
        "        self_attn_probs = []\n",
        "        for layer in self.layers:\n",
        "            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq)\n",
        "            dec_inputs, self_attn_prob = layer(dec_outputs, dec_self_attn_mask)\n",
        "            self_attn_probs.append(self_attn_prob)\n",
        "        # (bs, n_dec_seq, d_hidn), [(bs, n_dec_seq, n_dec_seq)]\n",
        "        return dec_outputs, self_attn_probs"
      ],
      "metadata": {
        "id": "CcpqPHkGN79t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. GPT\n",
        "\n",
        "GPT는 단순히 Transformer Decoder를 실행합니다.\n",
        "Pretrain된 모델을 저장하기위한 save, 저장된 모델을 읽기위한 load 함수가 추가로 정의 되었습니다"
      ],
      "metadata": {
        "id": "q076gdZAJl48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"GPT\"\"\"\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.decoder = Decoder(self.config)\n",
        "\n",
        "    def forward(self, dec_inputs):\n",
        "        # (bs, n_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)]\n",
        "        dec_outputs, dec_self_attn_probs = self.decoder(dec_inputs)\n",
        "        # (bs, n_dec_seq, n_dec_vocab), [(bs, n_head, n_dec_seq, n_dec_seq)]\n",
        "        return dec_outputs, dec_self_attn_probs\n",
        "\n",
        "    def save(self, epoch, loss, path):\n",
        "        torch.save({\n",
        "            \"epoch\" : epoch,\n",
        "            \"loss\" : loss,\n",
        "            \"state_dict\" : self.state_dict()\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        save = torch.load(path)\n",
        "        self.load_state_dict(save[\"state_dict\"])\n",
        "        return save[\"epoch\"], save[\"loss\"]"
      ],
      "metadata": {
        "id": "2OqWvEJ8Jo48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Pretrain Model\n",
        "\n",
        "![](https://paul-hyun.github.io/assets/2019-12-30/pretrain.png)\n",
        "<center></center>\n",
        "\n",
        "GPT를 Pretrain 하기위한 클래스 입니다.\n",
        "GPTPretrain 클래스의 목적은 입력 단어에 대한 다음 단어를 예측 하는 겁니다.\n",
        "\n",
        "1. GPT의 결과를 입력으로 단어를 예측하기 위한 projection_lm를 선업합니다. (줄 : 10)\n",
        "\n",
        "2. projection_lm은 Decoder의 Embedding과 weight를 share합니다. (줄 : 11)\n",
        "\n",
        "3. GPT 실행 결과를 입력으로 projection_lm을 실행해서 단어를 예측하도록 합니다. (줄 : 17)\n",
        "\n",
        "4. 입력에 대한 다음 단어를 예측하는 것이므로 결과의 마지막을 제외한 나머지를 리턴합니다. (줄 : 19)\n"
      ],
      "metadata": {
        "id": "tR1fP8oGLwE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" GPT pretrain \"\"\"\n",
        "\n",
        "class GPTPretrain(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init()\n",
        "        self.config = config\n",
        "\n",
        "        self.gpt = GPT(self.conig)\n",
        "        # lm\n",
        "        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_dec_vocab, bias=False)\n",
        "        self.projection_lm.weight = self.gpt.decoder.dec_emb.weight\n",
        "\n",
        "    def forward(self, dec_inputs):\n",
        "        # (bs, n_dec_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)]\n",
        "        dec_outputs, dec_self_attn_probs = self.gpt(dec_inputs)\n",
        "        # (bs, n_dec_seq, n_dec_vocab)\n",
        "        logits_lm = self.projection_lm(dec_outputs)\n",
        "        # (bs, n_dec_seq - 1, n_dec_vocab), (bs, n_output), [(bs, n_head, n_dec_seq, n_dec_seq)]\n",
        "        return logits_lm[:, :-1, :].contiguous(), dec_self_attn_probs"
      ],
      "metadata": {
        "id": "g8VgZ3XTMCRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Pretrain Data 생성\n",
        "\n",
        "단락별 pretrain 데이터 생성 함수\n",
        "단락을 여러 개의 Pretrain 데이터로 만드는 함수 입니다.\n",
        "\n",
        "1. 입력의 시작은 [BOS] 끝은 [EOS]입니다. tgt_seq는 n_seq에서 2개를 뺀 값입니다. (줄 : 5, 6)\n",
        "\n",
        "2. 단락을 줄 단위로 for loop를 돌며 아래내용 (줄 : 4 ~ 7)을 실행합니다. (줄 : 11)\n",
        "\n",
        "3. current_chunk에 line을 추가, current_length에 라인의 tokens 수를 더합니다. (줄 : 12, 13)\n",
        "\n",
        "4. 마지막 줄 이거나 current_length가 tgt_seq를 넘을 경우 학습데이터를 만듭니다. (줄 : 14)\n",
        "\n",
        "5. current_chunk의 값을 tokens로 만들고 tgt_seq를 초과하는 부분은 제거합니다. (줄 : 17, 18)\n",
        "\n",
        "6. [BOS] + tokens + [EOS] 형태로 데이터를 생성합니다. (줄 : 21)"
      ],
      "metadata": {
        "id": "sA-kFC6XPIhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" doc별 pretrain 데이터 생성 \"\"\"\n",
        "\n",
        "def create_pretrain_instances(doc, n_seq):\n",
        "    # for [BOS], [EOS]\n",
        "    max_seq = n_seq - 2\n",
        "    tgt_seq = max_seq\n",
        "\n",
        "    instances = []\n",
        "    current_chunk = []\n",
        "    current_lenght = []\n",
        "    for i in range(len(doc)):\n",
        "        current_chunk.append(doc[i]) # line\n",
        "        current_lenght += len(doc[i])\n",
        "        if i == len(doc) - 1 or current_lenght >= tgt_seq:\n",
        "            if 0 < len(current_chunk):\n",
        "                tokens = []\n",
        "                for chunk in current_chunk: tokens.extend(chunk)\n",
        "                tokens = tokens[:tgt_seq]\n",
        "                if 1 < len(tokens):\n",
        "                    instance = {\n",
        "                        \"tokens\" : [\"[BOS]\"] + tokens + [\"[EOS]\"],\n",
        "                    }\n",
        "                    instances.append(instance)\n",
        "            current_chunk = []\n",
        "            current_lenght = 0 \n",
        "    return instances"
      ],
      "metadata": {
        "id": "IFDl4lw1PMo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretrain 데이터 생성 함수\n",
        "말뭉치를 읽어 Pretrain 데이터를 만드는 함수 입니다."
      ],
      "metadata": {
        "id": "_b0MOdugRnFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" pretrain 데이터 생성 \"\"\"\n",
        "\n",
        "def make_pretrain_data(vocab, in_file, out_file, n_seq):\n",
        "    line_cnt = 0\n",
        "    with open(in_file, \"r\") as in_f:\n",
        "        for line in in_f:\n",
        "            line_cnt += 1\n",
        "\n",
        "    docs = []\n",
        "    with open (in_file, \"r\") as f:\n",
        "        doc = []\n",
        "        with tqdm(total = line_cnt, desc = f\"Lodaing\") as pbar:\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if 0 < len(doc):\n",
        "                        docs.append(doc)\n",
        "                        doc = []\n",
        "                else:\n",
        "                    pieces = vocab.encode_as_pieces(line)\n",
        "                    if 0 < len(pieces):\n",
        "                        doc.append(pieces)\n",
        "                    "
      ],
      "metadata": {
        "id": "X-VIFIzwTCEx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}